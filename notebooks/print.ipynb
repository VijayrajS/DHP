{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://theprint.in/?s=CAB\"\n",
    "r1 = requests.get(url)\n",
    "coverpage = r1.content\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "news_items = soup1.find_all('div', class_='item-details')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in news_items:\n",
    "    d ={}\n",
    "    d['link'] = i.find('a')['href']\n",
    "    d['text'] = i.text\n",
    "    article_links.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://theprint.in/india/hope-european-parliament-move-against-caa-are-followed-by-sanctions-tweets-mehbooba/355132/',\n",
       " 'text': '\\n                Hope European Parliament move against CAA are followed by sanctions, tweets Mehbooba\\n                \\n                                        PTI -                     27 January, 2020                                    \\n\\n                \\n                     with sanctions.\\n\\n\"Relieved to see EU parliament pass resolutions against communal CAB                \\n            '}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "article_links = []\n",
    "while count<55:\n",
    "    print(count)\n",
    "    url = \"https://theprint.in/page/\" + str(count) + \"/?s=NRC\"\n",
    "    r1 = requests.get(url)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    news_items = soup1.find_all('div', class_='item-details')\n",
    "    for i in news_items:\n",
    "        d ={}\n",
    "        d['link'] = i.find('a')['href']\n",
    "        d['text'] = i.text\n",
    "        article_links.append(d)\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq  = []\n",
    "for i in article_links:\n",
    "    if i not in uniq:\n",
    "        uniq.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "fin_articles = []\n",
    "counter = 0\n",
    "max_c = len(uniq)\n",
    "for i in uniq:\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n",
    "    url = i['link']\n",
    "    r1 = requests.get(url)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    texts = []\n",
    "    ps = soup1.find_all('p')\n",
    "    for j in range(len(ps)):\n",
    "\n",
    "        if j > len(ps) - 8:\n",
    "            break\n",
    "        texts.append(ps[j].text)\n",
    "    text = \"\\n\".join(texts)\n",
    "    d = {}\n",
    "    try:\n",
    "        d['text'] = text\n",
    "        d['link'] = i['link']\n",
    "        d['time'] = soup1.find('span',class_=\"update_date\").text\n",
    "        fin_articles.append(d)\n",
    "    except:\n",
    "        print(i)\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('print_nrc.pickle', 'wb') as handle:\n",
    "    pickle.dump(fin_articles, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Res",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
